%!TEX root = ../dokumentation.tex

\chapter{Projektabschluss, Fazit \& Ausblick}\label{cha:Schlussbetrachtung}
Bevor die Anwendung in Betrieb genommen werden kann, muss eineaAbschließende Beurteilung durchgeführt werden. Dabei wird zunächst festgestellt, ob der Erfüllungsgrad der fertigen Applikation für eine Inbetriebnahme ausreichend ist.

Aufbauend auf dem Abgleich der Ziele mit der Anwendung folgt ein abschließendes Fazit über das gesamte Projekt. Dabei wird nochmals kritisch betrachtet, ob auch die Zielsetzung der Arbeit erreicht wurde. Außerdem werden Kernpunkte und Erkenntnisse aus der Dokumentation zusammengefasst.

Zuletzt folgt ein Ausblick auf mögliche Weiterentwicklungen der Applikation, sowie weiterführende Maßnahmen, welche nicht Bestandteil dieser Arbeit waren. Dabei handelt es sich sowohl um Maßnahmen, welche vor Inbetriebnahme der Anwendung erfolgen müssen, als auch um optionale Maßnahmen zur Optimierung der Funktionalitäten und Performance. 

%<Projektabschluss. Worum geht es hier? Warum muss eine Schlussbetrachtung gemacht werden? Welche Schritte werden dabei durchgeführt?>

\section{Vergleich der Anwendung mit den Zielen}
Der Erfolg (oder Misserfolg) eines Projektes wird i.d.R. an dem sog. Erfüllungsgrad gemessen. Dieser wird bestimmt indem ein Abgleich der zuvor definierten Ziele mit den Funktionen der fertigen Anwendung vorgenommen wird.

Im Folgenden soll der Erfüllungsgrad für die vorliegende Anwendung bestimmt werden. Dabei muss beachtet werden, dass dieser lediglich eine Aussage über die Anwendung, nicht jedoch über die geschriebene Arbeit selbst trifft. Der Erfolg (oder Misserfolg) der Arbeit selbst wird in \autoref{sec:Fazit} festgestellt.

\subsection{Abgleich der funktionalen Anforderungen}
Im Folgenden werden die in \autoref{subsec:Anforderungen} definierten, funktionalen Anforderungen mit der fertigen Anwendung abgeglichen. Die Bewertung wird in drei Stufen durchgeführt. Diese sind erfüllt (3), teilerfüllt (2) und unerfüllt (1).

\subsubsection{A001 - Formatunabhängige Analyse (3)}
Die Verarbeitung der Logfiles wird ohne eine Umformung der Einträge durchgeführt. Es sind alle textbasierten Dateiformate analysierbar. Komplexere Daten sind, unter Implementierung einer entsprechenden \textit{InputFormat} Klasse, ebenfalls einlesbar.

\subsubsection{A010 - Dynamisches Analyseverfahren (3)}
Die Parameter der Analyse sind über die Konfigurationsschnittstelle flexibel. Suchmuster und Mappingverfahren sind über die entsprechenden Properties frei konfigurierbar.

\subsubsection{A020 - Konfigurierbarkeit (3)}
Die Anwendung ist über drei Stufen frei konfigurierbar. Es werden Dateien des Typs \textit{.properties} verwendet. Die Standardeinstellungen sind für jeden Programmlauf individuell anpassbar.

\subsubsection{A030 - Erweiterbarkeit (3)}
Durch die modulare Architektur des Programms und der hohen Flexibilität durch die Konfigurationsschnittstelle, sind Erweiterungen schnell und einfach implementierbar. Da die Angabe der zu verwendenden Klassen immer mit dem vollständigen Paket erfolgen muss, ist ein Einbinden und Verwenden von externen Bibliotheken ohne Einschränkungen möglich.

\subsection{Abgleich der Produktleistungen}
Nach den funktionalen Anforderungen muss ebenfalls ein Abgleich der Anwendung mit den in \autoref{subsec:Produktleistungen} definierten Produktleistungen erfolgen. Dabei werden die gleichen Stufen wie beim Abgleich der funktionalen Anforderungen verwendet.

\subsubsection{PL001 - Ausführungszeit (2)}
Die maximale Laufzeit der Anwendung wurde durch \autoref{equ:MaxAusführungszeit} in \autoref{subsubsec:PL001} definiert. Nach der Entwicklung wurde, durch die Durchführung von Anwendungstests, eine Minimallaufzeit von $\approx 8$ Sekunden ermittelt. Dies ist auf einen konstanten Zeitfaktor zurückzuführen, welcher bedingt wird durch das Starten der Anwendung innerhalb des Hadoop Frameworks.

Aus diesem Grund wird diese Produktleistung lediglich als teilerfüllt angesehen. Für Analysen mit einem Intervall von weniger als 40 Sekunden wird die maximale Laufzeit überschritten.

\subsubsection{PL010 - Dateigröße (3)}
Die Vorgabe Logfiles mit einer Größe von bis zu 500 \ac{MB} ohne einen Split zu verarbeiten wird vollständig erfüllt. Erst ab einer Größe von ca. 1 \ac{GB} werden durch die Anwendung Splits erzeugt und parallel verarbeitet. 

\subsection{Zusammenfassung}
Abschließend wurde festgestellt, dass alle funktionalen Anforderungen vollständig erfüllt werden konnten. Bei den Produktleistungen gibt es lediglich eine Teilerfüllung bei der maximalen Anwendungslaufzeit.

Die entwickelte Anwendung genügt den definierten Zielen. Das Projekt kann somit als erfolgreich gewertet werden. 

%<Welche Ziele wurden erfüllt? Welche nicht? Alles bezogen auf die Anwendung nicht auf die Bachelor Arbeit! Wo gab es Abweichungen? Wenn ja, warum gab es die?>

%\section{Proof of Concept}
%<Dokumentation einer kompletten Durchführung der Anwendung von Anfang bis Ende. Aufzeigen der Ergebnisse. Funktioniert die Anwendung?>

\newpage
\section{Fazit}\label{sec:Fazit}
Das Ziel der Arbeit durch die Anwendung des MapReduce Verfahrens schneller bessere Informationen über den Zustand des Systems zu erhalten, konnte nicht vollständig erfüllt werden. Es wurde zwar eine prototypische Anwendung geschrieben, welche mittels MapReduce textbasierte Logfiles analysieren kann, jedoch nicht in einem dem Ziel entsprechenden Zeitraum.

Bei den durchgeführten Anwendungstests wurde ein konstanter Faktor von $\approx 8$ Sekunden festgestellt, welcher durch das Starten der Anwendung in Hadoop verursacht wird. In Verbindung mit dem Ziel, dass die Laufzeit der Analyse maximal $\sfrac{1}{5}$ des Ausführungsintervalls betragen sollte, bedeutet dies, dass nur Analysen mit einem Intervall von 40 Sekunden oder größer durchgeführt werden können. Dies ist für eine Real-/Neartime Analyse ein zu großer Intervall.

Es konnte jedoch gezeigt werden, dass sich mit steigender Anzahl der zu verarbeitenden Einträge Ausführungskosten und Effizienz von MapReduce stark verbessern. Die Verarbeitung von sehr großen Datenmengen ($\geq10^6$) ist die eigentliche Stärke des Modells.

Durch die Entwicklung der Applikation wurden nicht nur wertvolle Kenntnisse über die Funktionsweise des MapReduce Modells gewonnen, sondern auch Erfahrungen für die Arbeit im BigData-Bereich gesammelt.

Zudem wird durch die Anwendung anstatt einer Verbesserung der Real-/Neartime Analyse der Infrastruktur ein neues Werkzeug für Langzeitanalysen bereitgestellt (Analysen mit einem Intervall $\geq$ 40 Sekunden durchgeführt werden).

Durch die modulare Entwicklung lässt sich die Applikation leicht erweitern. Die generische Struktur der Konfiguration ermöglicht zudem individuelle Anpassungen, wodurch eine Vielzahl unterschiedlicher Informationen gewonnen werden können.

Obwohl das eigentliche Ziel nicht erreicht wurde, kann die Arbeit aufgrund der gewonnenen Erkenntnisse und der entwickelten Applikation als Erfolg angesehen werden. Sowohl das Wissen über die Arbeitsweise von MapReduce als auch die Anwendung selbst stellen wertvolle Werkzeuge für zukünftige Analysen dar.

%<Fazit ziehen über das Projekt und die Arbeit. Welche Erkenntnisse wurden gewonnen? Was hat gut/schlecht funktioniert? Wurden die eigenen Erwartungen erfüllt oder nicht? War das Projekt erfolgreich?>

\newpage
\section{Ausblick}\label{sec:Ausblick}
Vor Inbetriebnahme des Logfileanalyzers sollte eine Laufzeitanalyse, nach dem in \autoref{subsec:Laufzeitanalyse} beschriebenen Verfahren, auf dem Zielsystem durchgeführt werden. Daraus sollten sich neue Erkenntnisse bezüglich der Geschwindigkeit von MapReduce auf einem tatsächlichen System ergeben und somit das Einsatzgebiet des Programms klar werden.

Um das vollständige Potenzial der Anwendung ausnutzen zu können, sollte eine Änderung der Infrastruktur vorgenommen werden. Wie in \autoref{subsec:Infrastruktur} beschrieben wurde, werden die Logfiles aktuell auf einem \ac{NFS} abgelegt und über einen Mount an den Infrastrukturserver gebunden. Dies hat den Nachteil, dass die zu analysierenden Daten zunächst in ein dort laufendes \ac{HDFS} übertragen werden müssen.

Aus diesem Grund sollte das bestehende \ac{NFS} durch ein \ac{HDFS} ersetzt werden. Wenn dies erfolgt ist kann die Grundidee von Hadoop, die Anwendung zu den Daten zu bringen statt die Daten zur Applikation, vollständig umgesetzt werden.

Um spezifischere Analysen durchführen zu können, sollten weitere \textit{Mapper}, sowie \textit{InputFormat} und \textit{RecordReader} Klassen entwickelt werden, da diese die Grundlage für neue Analyseverfahren und der Verarbeitung weiterer Dateiformate bilden.

%<Welche Möglichkeiten bieten sich in der Zukunft? Was kann noch erreicht werden? Was bleibt offen? Welche nächsten Schritte sollte man gehen?>