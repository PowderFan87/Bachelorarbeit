%!TEX root = ../dokumentation.tex

\chapter{Umsetzung}\label{cha:Umsetzung}
<Allgemeine Beschreibung des Inhalts des Kapitels.>

\section{Infrastruktur \& Prozesse}
Vor Beginn der Entwicklung werden im folgenden die Umgebung, in welcher die Applikation später ausgeführt werden soll, sowie der Anwendungsprozess, genauer definiert. 

<Weitere Einführung in das Kapitel bzw. aktuelle überarbeiten>

\subsection{Beschreibung der Infrastruktur}
<...Einleitung/überleitung zur beschreibung der infrastruktur>

Alle Server (sowohl Wen- wie auch Application-Server), schreiben ihre jeweiligen Logfiles auf ein \ac{NFS}. Der entsprechende Infrastruktur-Server mounted die benötigten Verzeichnisse. Der Server verfügt über eine Hadoop Installation, auf welcher die Applikation ausgeführt wird (siehe \autoref{fig:AufbauInfrastruktur}).

\begin{figure}
	\centering
	\includegraphics[width=.8\textwidth]{Infrastruktur.png}
	\caption{Aufbau der Infrastruktur}
	\label{fig:AufbauInfrastruktur}
\end{figure}

\subsection{Definition des Anwendungsprozesses}


\begin{figure}
	\centering
	\includegraphics[scale=1]{PAP_Main_main.png}
	\caption{PAP für main Methode der Main Klasse}
	\label{fig:PAP_Main_main}
\end{figure}

<Hier sollte der Prozess der Anwendung geplant werden. wo liegen die Daten? wie findet der zugriff statt? nach welchem schema sind sie abgelegt und benannt? Außerdem sollte klar werden wie die Informationen verarbeitet werden. ein großer map reduce oder eine verkettung mehrerer mapper und reducer? Außerdem muss das ausgabeformat festgelegt werden. Bzw. zwei alternativen. eine für wenn die anwendung performant genug ist um als check skript zu laufen, und eine für den fall das nicht.>

\section{Die Konfigurationsschnittstelle}
Wie bereits bei der Beschreibung des Verarbeitungsprozesses gezeigt wurde, wird direkt nach dem Start der Anwendung eine Initialisierung vorgenommen. Hierbei soll die Umgebung so konfiguriert werden, dass die gewünschte Analyse durch die Anwendung ohne Probleme vorgenommen werden kann.

Die Konfiguration der Anwendung wird durch Dateien vom Typ \textit{.properties} vorgenommen. Diese werden von Java vollständig unterstützt. Für die Interpretation der Konfigurationsdateien wird auf die Klasse \textit{java.util.Properties} zurückgegriffen. Die Funktionsweise der Properties wird im folgenden genauer betrachtet.

\subsection{Aufbau von Properties}
Die Konfigurationswerte werden in Properties Dateien als Schlüssel-Wert-Paare gespeichert. Die Trennung kann hierbei durch einen Doppelpunkt oder Gleichheitszeichen erfolgen. Des weiteren ist es möglich, Platzhalter bei den Werten zu definieren, welche später durch Variablen ersetzt werden können. Außerdem ist es möglich, zum besseren Verständnis, die Datei um Kommentare zu erweitern. Jede Zeile, welche mit einem Hash oder Ausrufezeichen beginnt, wird als Kommentar gesehen, und von der Anwendung nicht interpretiert. \autoref{lis:AuszugDefaultProperties} zeigt einen Auszug aus der \textit{default.properties} Datei. \\

\begin{lstlisting}[language=Bash,caption=Auszug aus default.properties,label=lis:AuszugDefaultProperties]
###
# Default properties for Logfileanalyzer.
# Properties can be extended by user defined properties.
# Never change this file to fit one case.
#

# Set mode for execution (DEBUG, TEST, LIVE)
RUNMODE         : DEBUG

# Runtime properties
LOGTARGET       : {0}

[...]
\end{lstlisting}

Grundsätzlich sind alle Konfigurationen vom Typ \textit{String}. Es ist, nativ, nicht möglich, direkt einen Wert in einem anderen Datentyp zu definieren. Falls eine Typenkonvertieren notwendig ist, muss diese manuell durchgeführt werden.

In Java stellt die Klasse \textit{Properties}, welche teil des \textit{java.util} Paketes ist, alle benötigten Methoden bereit. Um die Konfiguration der Anwendung zu vereinfachen, wurde die Klasse \textit{Configuration} im Paket \textit{com.hszuesz.logfileanalyzer} erzeugt. Diese leitet sich aus der Klasse \textit{Properties} ab, und stellt Erweiterungen zum einlesen mehrerer Konfigurationsdateien bereit. Dies ist notwendig, um eine stufenweise Konfiguration der Anwendung zu realisieren.

Da die Anwendung später innerhalb des Hadoop Frameworks ausgeführt wird, müssen die Konfigurationsdateien ein Teil der, durch Maven erzeugten, JAR-Datei sein. Dies wird durch eine Ergänzung in der \textit{pom.xml} innerhalb des \textit{build} Knotens sichergestellt (siehe \autoref{lis:POMErgänzung}). \\

\begin{lstlisting}[language=XML,caption=pom.xml Ergänzung für Konfigurationsdateien,label=lis:POMErgänzung]
[...]
	<resources>
		<resource>
			<directory>conf</directory>
			<includes>
				<include>*.properties</include>
			</includes>
		</resource>
	</resources>
[...]
\end{lstlisting}

\subsection{Beschreibung der Konfigurationsstufen}
Die Konfiguration der Anwendung wird in drei Stufen durchgeführt. Dies soll die Komplexität der Konfigurationsdateien reduzieren, indem die individuellen Anpassungen für jede Ausführung des Programms gekapselt, und immer gleiche Einstellungen ausgelagert werden.

Die erste Stufe bilden die sog. Core-Properties, welche in der Datei \textit{core.properties} hinterlegt sind. Wie der Name bereits erkennen lässt, handelt es sich hierbei um Grundlegende Einstellungen, welche den Kern der Anwendung beeinflussen. Dazu gehört z.B. die Konfiguration der verschiedenen \textit{RUNMODES} oder Pfade zu weiteren wichtigen Dateien, wie den Default- oder Logger-Properties. Ein überschreiben dieser Einstellungen ist nicht möglich.

Die zweite Stufe bildet die Defaults. Hier werden alle Konfigurationen vorgenommen, welche für eine Standardausführung der Anwendung benötigt werden. Alle Einstellungen, welche in der \textit{defaults.properties} Datei hinterlegt sind, können durch den Anwender verändert werden.

Die dritte und letzte Stufe bilden die User-Properties. Beim Start der Anwendung kann der Pfad zu einer individuellen Properties-Datei übergeben werden. In dieser können die Einstellungen, welche durch die Defaults vorgenommen wurden, ergänzt und  überschreiben werden.

Der Prozess für die Verarbeitung der einzelnen Stufen wird im \ac{PAP} der \textit{LFAConfiguration} verdeutlicht, welcher im Anhang zu finden ist.

%<Beschreibung wie Properties programmiert werden. Wie werden diese in der Anwendung umgesetzt? Welche Rolle spielen Properties für den generischen Teil der Anwendung?>

\subsection{Logger Konfiguration}
<Beschreibung wie der Logger in Java funktioniert und wie dieser hier eingesetzt wird. Speziell die Konfiguration über die logger.properties datei hervorheben.>

%\section{Grundlagen für Datenverarbeitung}
%<Beschreibung der Entwicklung für die Grundlagen zur Datenverarbeitung. Welche Klassen werden dabei verwendet? Welches System liegt dahinter? Warum dieses System? Dabei nicht nur auf die Speicherung von Daten eingehen sondern auch auf das Lesen von Dateien.>

\section{Bestimmung des Aufbaus der Logfiles}
<Wie sehen die Logfiles aus? Welche Formate haben sie? Welche rolle spielen diese bei der Datenverarbeitung? Welche Informationen sind die richtigen Informationen?>

\section{Implementierung von MapReduce}
Nachdem alle Grundlagen der Anwendung fertiggestellt wurden kann mit der Entwicklung des Kernstückes begonnen werden, der Implementierung von MapReduce. Dabei besteht die größte Hürde im generischen Ansatz, welcher, insbesondere im Kern des Programms, immer berücksichtigt werden muss.



\subsection{Einblick in die InputFormat Klasse}


\subsection{Der RecordReader}


\subsection{Mapper und Reducer}

<Beschreibung vom Kern der Anwendung. Wie wird der Algorithmus umgesetzt? Welche Klassen/Methoden sind notwendig? Wie unterscheidet sich die Implementierung bei unterschiedlichem Input. Spielt das überhaupt eine Rolle oder muss es nur Text sein? Welches Ergebnis bekommt man und in welcher Form?>

\section{Ausführungsdatei Logfileanalyzer.sh}

\begin{lstlisting}[language=Bash,caption=Ausführungsdatei Logfileanalyzer.sh,label=lis:Logfileanalyzer.sh]
#!/bin/bash

#Load config
. $1/config.cfg
#Cleanup input direktory on dfs
/bin/hdfs dfs -rm -r -skipTrash input/*
#Delete output directory on dfs
/bin/hdfs dfs -rm -r -skipTrash output
#Put input files on dfs
/bin/hdfs dfs -put $MAP_REDUCE_INPUT input
mkdir $MAP_REDUCE_OUTPUT
if [ $MAP_REDUCE_USE_USER_PROPERTIES == true ]
then
        #Export HADOOP_OPTS to pass user properties file
        export HADOOP_OPTS="-Dlfa.userconf=$1/$MAP_REDUCE_USER_PROPERTIES"
fi
if [ $MAP_REDUCE_USE_LOG == true ]
then
        #Use log logfile
        echo "Log file wird verwendet"
        /bin/hadoop jar $1/$MAP_REDUCE_JAR $MAP_REDUCE_MAIN input output 2> $MAP_REDUCE_LOGFILE
else
        echo "Kein Logfile"
        /bin/hadoop jar $1/$MAP_REDUCE_JAR $MAP_REDUCE_MAIN input output
fi
if [ $MAP_REDUCE_USE_USER_PROPERTIES == true ]
then
        #Unset HADOOP_OPTS after apllication finished
        export HADOOP_OPTS=""
fi
#Get output from dfs
/bin/hdfs dfs -get output/* $MAP_REDUCE_OUTPUT
if [ $MAP_REDUCE_DISPLAY_OUTPUT == true ]
then
        cat ${MAP_REDUCE_OUTPUT}/*
fi
\end{lstlisting}

<Beschreiben der Bash Datei, durch welche der job (Jar) ausgeführt wird. Beschreiben der einzlenen Schritte und der configuration>

\section{Anwendungstest \& Auswertung der Ergebnisse}

\begin{lstlisting}[language=Java,caption=Properties für Anwendungstest,label=lis:PropertiesAnwendungstest]
lfa.runmode              : DEBUG

lfa.driver.mapper        : com.hszuesz.logfileanalyzer.mapper.PatternMapper
lfa.driver.reducer       : com.hszuesz.logfileanalyzer.reducer.CountReducer

lfa.driver.input.format  : org.apache.hadoop.mapreduce.lib.input.TextInputFormat

lfa.driver.output.key    : org.apache.hadoop.io.Text
lfa.driver.output.value  : org.apache.hadoop.io.IntWritable
lfa.driver.output.format : org.apache.hadoop.mapreduce.lib.output.TextOutputFormat

lfa.driver.job.name      : Laufzeittest webadapter.log

lfa.mapper.pattern.key   : \\] ([A-Z]{4,5})
lfa.mapper.nomatchaction : SKIP
\end{lstlisting}

\subsection{Analyse der Laufzeiten}
Die im Anwendungstest erhobenen Laufzeiten sollen nun weiter Analysiert werden. Hierfür soll zunächst das Ausmaß des \textit{linearen} Zusammenhangs zwischen der Anzahl der verarbeiteten Einträge und der Laufzeit ermittelt werden. Gerald und Susanne Teschl definieren die, nach dem englischen Mathematiker Karl Pearson (1857 - 1936) benannte, Kennzahl wie folgt:

\flqq Gegeben seien die Wertepaare $(x_1,y_1), \dots,(x_n,y_n)$, wobei nicht alle $x_i$ gleich sind bzw. nicht alle $y_i$ gleich sind. Die Zahl
\begin{equation*}
r_{xy} = \frac{s_{xy}}{s_x \cdot s_y}
\end{equation*} 
heißt \textbf{(empirischer) Korrelationskoeffizient} oder \textbf{Pearson'scher Korrelationskoeffizient}. Dabei ist
\begin{equation*}
s_{xy} = \frac{1}{n-1} \displaystyle\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
\end{equation*}
die \textbf{(empirische) Kovarianz}, $\bar{x}$, $\bar{y}$ sind die arithmetischen Mittelwerte und
\begin{equation*}
s_x = \sqrt{\frac{1}{n-1} \displaystyle\sum_{i=1}^{n} (x_i - \bar{x})^2}, \quad s_y = \sqrt{\frac{1}{n-1} \displaystyle\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{equation*}
sind die (empirischen) Standardabweichungen der $x_i$ bzw. der $y_i$-Werte.\frqq\footcite[S. 213]{Teschl.2014}

Der Pearson'sche Korrelationskoeffizient liegt immer zwischen $-1$ und $+1$. Je näher $r_{xy}$ an $-1$ oder $1$ liegt, desto genauer konzentrieren sich die Datenpunkte auf einer Geraden. Zudem spricht man bei einem Korrelationskoeffizienten $r_{xy}>0$ von einer \textbf{positiven (linearen) Korrelation}.\footcite[Vgl.][S. 214]{Teschl.2014}

Nach Anwendung der eben definierten Gleichungen auf die, durch den Anwendungstest, ermittelten Werte, ergibt sich ein Wert von $r_{xy} \approx 0,9982$. Daraus folgt eine starke, lineare Korrelation zwischen der Laufzeit und der Anzahl der verarbeiteten Einträge.

Basierend auf dieser Abhängigkeit lässt sich, durch die sog. \textbf{Regressionsanalyse}, eine Funktion bestimmen, mit welcher, anhand der vorhandenen Testdaten, die zu erwartende Laufzeit der Anwendung, in Abhängigkeit zur Menge der zu verarbeitenden Daten, approximiert werden kann.

Bei der \textbf{linearen Regression} wird eine Funktion der Form $y = f(x) = kx + d$ gesucht (der sog. \textbf{Regressionsgeraden}), welche der folgenden Definition gerecht wird:

\flqq Die Gerade $f(x) = kx + d$, für die
\begin{equation*}
\displaystyle\sum_{i=1}^{n} (y_i - f(x_i))^2
\end{equation*}
minimal wird, ist gegeben durch
\begin{equation*}
k = r_{xy} \frac{s_y}{s_x}, \quad \quad d = \bar{y} - k\bar{x}.
\end{equation*}
Hier ist $r_{xy}$ der empirische Korrelationskoeffizient, $\bar{x}$, $\bar{y}$ sind die arithmetischen Mittelwerte und $s_x$, $s_y$ die Standardabweichungen der Stichprobenwerte $x_i$ bzw. $y_i$.\frqq\footcite[216]{Teschl.2014}

Daraus ergeben sich, basierend auf den ermittelten Testdaten, die Werte $k \approx 8,199 \cdot 10^{-6}$, $d \approx 8,418$, und die \autoref{equ:Laufzeit}, für die Berechnung der Laufzeit. \autoref{fig:VerlaufRegressionsgerade} zeigt den Verlauf der Regressionsgeraden durch die Testdaten.

\begin{equation}
t_e = f(x) = 8,199 \cdot 10^{-6}x + 8,418 \quad \quad \quad \{x \in \mathbb{N}\} \label{equ:Laufzeit}
\end{equation}

\begin{figure}[h]
	\includegraphics[width=1\textwidth]{Laufzeitanalyse.png}
	\caption{Verlauf der Regressionsgeraden}
	\label{fig:VerlaufRegressionsgerade}
\end{figure}

Um die ermittelte Funktion zu testen, wird nun zunächst die Laufzeit für die Verarbeitung von 20 Mio. Einträge mit \autoref{equ:Laufzeit} berechnet. Anschließend wird die Anwendung mit der entsprechenden Anzahl von Einträgen zehn mal ausgeführt (siehe \autoref{lis:Laufzeit20Mio}), und die durchschnittliche Laufzeit mit der Berechnung verglichen. \\

\begin{lstlisting}[language=Bash,caption=Laufzeiten mit 20 Mio. Einträgen,label=lis:Laufzeit20Mio]
Runtime: 185
Runtime: 171
Runtime: 166
Runtime: 174
Runtime: 166
Runtime: 172
Runtime: 170
Runtime: 170
Runtime: 171
Runtime: 171
\end{lstlisting}

Die Berechnete Laufzeit beträgt $t_e = 172,398$. Die durchschnittliche Laufzeit, bei 20 Mio. Einträgen, beträgt $171,6$. Die berechnete Laufzeit weicht lediglich $\approx 0,46\%$ vom Durchschnitt ab, was die Richtigkeit der Funktion hinreichend bestätigt.

\subsection{Definition des minimalen Ausführungsintervalls}
In \autoref{subsubsec:PL001} wurde, durch die \autoref{equ:MaxAusführungszeit}, ein maximale Laufzeit für einen festen Ausführungsintervall definiert. Durch die Kombination mit \autoref{equ:Laufzeit} lässt sich die Definition um einen Gültigkeitsbereich für $t_i$ erweitern:

\begin{equation}
t_e \leq \frac{t_i}{5} \quad \quad \quad \{t_e \in \mathbb{Q}^+\},\;\{t_i \in \mathbb{N}\;|\;5t_e \leq t_i \leq +\infty\} \label{equ:MinInterval}
\end{equation}

Durch diese Anpassung lässt sich, bereits vor Implementierung einer Analyse, ein minimaler Ausführungsintervall, in Abhängigkeit zur Datenmenge, bestimmen. Dies kann auch für bereits bestehende Analysen durchgeführt werden, um festzustellen, ob diese durch die Anwendung ersetzt werden können.

%<IDEE: Es wurde eine formel angegeben, welche aussage über die maximale ausführungsdauer gibt...eventuell lassen sich diese beiden formeln verbinden (also eine Formel die aussage gibt über den maximal möglichen intervall für eine bestimmte datenmenge \autoref{equ:MinInterval})>

%\begin{figure}
%	\includegraphics[width=1\textwidth]{Zeit_Pro_Eintrag.png}
%	\caption{Verarbeitungszeit pro Eintrag}
%	\label{fig:VerarbeitungszeitProEintrag}
%\end{figure}

%<Dokumentation der Tests mit unterschiedlich großen Datenmengen (10 Logeinträge bis 10.000.000 Einträge). Grafische Darstellung der Laufzeiten. Jede Stufe min. 10 mal ausführen und Ausführungszeit Nottieren/Dokumentieren. In Kapitel 6 Erkenntnisse aus den Tests aufarbeiten>